{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f80495-b6cd-48c7-865f-ed3f620db4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/anderl23/hadoop-3.3.4/spark-3.3.1/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/anderl23/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:43:42,653 WARN [main] org.apache.spark.util.Utils - Your hostname, DESKTOP-SHBG2MR resolves to a loopback address: 127.0.1.1; using 192.168.0.115 instead (on interface wifi0)\n",
      "2022-12-15T03:43:42,663 WARN [main] org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:43:43,726 WARN [Thread-4] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Load cleaned_airline_data.csv into Spark Dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ml-delay').getOrCreate()\n",
    "df = spark.read.csv('cleaned_airline_data_v5.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0756f2cd-9b20-4bc1-b4b1-0d3b090a5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEPT_TIME: double (nullable = true)\n",
      " |-- DEPT_TIME: double (nullable = true)\n",
      " |-- DEP_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: double (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- ARR_DELAY: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: integer (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- FL_YEAR: integer (nullable = true)\n",
      " |-- FL_MONTH: integer (nullable = true)\n",
      " |-- FL_DOM: integer (nullable = true)\n",
      " |-- FL_DOW: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17eccfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35841068"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment to calculate row count\n",
    "#Show number of rows in dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8112e156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FL_DATE</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <td>108</td>\n",
       "      <td>114</td>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN</th>\n",
       "      <td>LAX</td>\n",
       "      <td>LAX</td>\n",
       "      <td>JFK</td>\n",
       "      <td>PHL</td>\n",
       "      <td>LAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEST</th>\n",
       "      <td>IAD</td>\n",
       "      <td>IAD</td>\n",
       "      <td>SFO</td>\n",
       "      <td>LAX</td>\n",
       "      <td>PHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_DEPT_TIME</th>\n",
       "      <td>700.0</td>\n",
       "      <td>2205.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPT_TIME</th>\n",
       "      <td>700.0</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <td>708.0</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>1116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <td>1411.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>1828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_IN</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <td>1445.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_TIME</th>\n",
       "      <td>1418.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>1837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <td>-27</td>\n",
       "      <td>-9</td>\n",
       "      <td>-26</td>\n",
       "      <td>14</td>\n",
       "      <td>-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCELLED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIVERTED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <td>285</td>\n",
       "      <td>280</td>\n",
       "      <td>405</td>\n",
       "      <td>360</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <td>258</td>\n",
       "      <td>272</td>\n",
       "      <td>380</td>\n",
       "      <td>374</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIR_TIME</th>\n",
       "      <td>243</td>\n",
       "      <td>247</td>\n",
       "      <td>356</td>\n",
       "      <td>352</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTANCE</th>\n",
       "      <td>2288</td>\n",
       "      <td>2288</td>\n",
       "      <td>2586</td>\n",
       "      <td>2402</td>\n",
       "      <td>2402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_YEAR</th>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_MONTH</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_DOM</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_DOW</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0                    1  \\\n",
       "FL_DATE              2013-01-01 00:00:00  2013-01-01 00:00:00   \n",
       "OP_CARRIER                            VX                   VX   \n",
       "OP_CARRIER_FL_NUM                    108                  114   \n",
       "ORIGIN                               LAX                  LAX   \n",
       "DEST                                 IAD                  IAD   \n",
       "CRS_DEPT_TIME                      700.0               2205.0   \n",
       "DEPT_TIME                          700.0               2204.0   \n",
       "DEP_DELAY                              0                   -1   \n",
       "TAXI_OUT                               8                   12   \n",
       "WHEELS_OFF                         708.0               2216.0   \n",
       "WHEELS_ON                         1411.0                523.0   \n",
       "TAXI_IN                                7                   13   \n",
       "CRS_ARR_TIME                      1445.0                545.0   \n",
       "ARR_TIME                          1418.0                536.0   \n",
       "ARR_DELAY                            -27                   -9   \n",
       "CANCELLED                              0                    0   \n",
       "DIVERTED                               0                    0   \n",
       "CRS_ELAPSED_TIME                     285                  280   \n",
       "ACTUAL_ELAPSED_TIME                  258                  272   \n",
       "AIR_TIME                             243                  247   \n",
       "DISTANCE                            2288                 2288   \n",
       "FL_YEAR                             2013                 2013   \n",
       "FL_MONTH                               1                    1   \n",
       "FL_DOM                                 1                    1   \n",
       "FL_DOW                                 3                    3   \n",
       "\n",
       "                                       2                    3  \\\n",
       "FL_DATE              2013-01-01 00:00:00  2013-01-01 00:00:00   \n",
       "OP_CARRIER                            VX                   VX   \n",
       "OP_CARRIER_FL_NUM                     11                  121   \n",
       "ORIGIN                               JFK                  PHL   \n",
       "DEST                                 SFO                  LAX   \n",
       "CRS_DEPT_TIME                      730.0                700.0   \n",
       "DEPT_TIME                          729.0                700.0   \n",
       "DEP_DELAY                             -1                    0   \n",
       "TAXI_OUT                              18                   14   \n",
       "WHEELS_OFF                         747.0                714.0   \n",
       "WHEELS_ON                         1043.0               1006.0   \n",
       "TAXI_IN                                6                    8   \n",
       "CRS_ARR_TIME                      1115.0               1000.0   \n",
       "ARR_TIME                          1049.0               1014.0   \n",
       "ARR_DELAY                            -26                   14   \n",
       "CANCELLED                              0                    0   \n",
       "DIVERTED                               0                    0   \n",
       "CRS_ELAPSED_TIME                     405                  360   \n",
       "ACTUAL_ELAPSED_TIME                  380                  374   \n",
       "AIR_TIME                             356                  352   \n",
       "DISTANCE                            2586                 2402   \n",
       "FL_YEAR                             2013                 2013   \n",
       "FL_MONTH                               1                    1   \n",
       "FL_DOM                                 1                    1   \n",
       "FL_DOW                                 3                    3   \n",
       "\n",
       "                                       4  \n",
       "FL_DATE              2013-01-01 00:00:00  \n",
       "OP_CARRIER                            VX  \n",
       "OP_CARRIER_FL_NUM                    124  \n",
       "ORIGIN                               LAX  \n",
       "DEST                                 PHL  \n",
       "CRS_DEPT_TIME                     1100.0  \n",
       "DEPT_TIME                         1104.0  \n",
       "DEP_DELAY                              4  \n",
       "TAXI_OUT                              12  \n",
       "WHEELS_OFF                        1116.0  \n",
       "WHEELS_ON                         1828.0  \n",
       "TAXI_IN                                9  \n",
       "CRS_ARR_TIME                      1915.0  \n",
       "ARR_TIME                          1837.0  \n",
       "ARR_DELAY                            -38  \n",
       "CANCELLED                              0  \n",
       "DIVERTED                               0  \n",
       "CRS_ELAPSED_TIME                     315  \n",
       "ACTUAL_ELAPSED_TIME                  273  \n",
       "AIR_TIME                             252  \n",
       "DISTANCE                            2402  \n",
       "FL_YEAR                             2013  \n",
       "FL_MONTH                               1  \n",
       "FL_DOM                                 1  \n",
       "FL_DOW                                 3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show first five rows in dataframe. (note this has been transposed in order to easily view)\n",
    "import pandas as pd\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea720d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:47:25,628 WARN [Thread-4] org.apache.spark.sql.catalyst.util.package - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "|            FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEPT_TIME|DEPT_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|FL_YEAR|FL_MONTH|FL_DOM|FL_DOW|OP_CARRIER_Index| OP_CARRIER_vec|OP_CARRIER_FL_NUM_Index|OP_CARRIER_FL_NUM_vec|ORIGIN_Index|      ORIGIN_vec|DEST_Index|        DEST_vec|\n",
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "|2013-01-01 00:00:00|        VX|              108|   LAX| IAD|        700.0|    700.0|        0|       8|     708.0|   1411.0|      7|      1445.0|  1418.0|      -27|        0|       0|             285|                258|     243|    2288|   2013|       1|     1|     3|            15.0|(19,[15],[1.0])|                  159.0|   (7154,[159],[1.0])|         4.0| (370,[4],[1.0])|      32.0|(369,[32],[1.0])|\n",
      "|2013-01-01 00:00:00|        VX|              114|   LAX| IAD|       2205.0|   2204.0|       -1|      12|    2216.0|    523.0|     13|       545.0|   536.0|       -9|        0|       0|             280|                272|     247|    2288|   2013|       1|     1|     3|            15.0|(19,[15],[1.0])|                  670.0|   (7154,[670],[1.0])|         4.0| (370,[4],[1.0])|      32.0|(369,[32],[1.0])|\n",
      "|2013-01-01 00:00:00|        VX|               11|   JFK| SFO|        730.0|    729.0|       -1|      18|     747.0|   1043.0|      6|      1115.0|  1049.0|      -26|        0|       0|             405|                380|     356|    2586|   2013|       1|     1|     3|            15.0|(19,[15],[1.0])|                 1054.0|  (7154,[1054],[1.0])|        18.0|(370,[18],[1.0])|       5.0| (369,[5],[1.0])|\n",
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "categoricalColumns = [\"OP_CARRIER\",\"OP_CARRIER_FL_NUM\",\"ORIGIN\",\"DEST\"]\n",
    "\n",
    "OP_CARRIER_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"OP_CARRIER_Index\")\n",
    "df_1 = OP_CARRIER_indexer.fit(df).transform(df)\n",
    "\n",
    "onehotencoder_OP_CARRIER_vector = OneHotEncoder(inputCol=\"OP_CARRIER_Index\", outputCol=\"OP_CARRIER_vec\")\n",
    "df_2 = onehotencoder_OP_CARRIER_vector.fit(df_1).transform(df_1)\n",
    "\n",
    "OP_CARRIER_FL_NUM_indexer = StringIndexer(inputCol=\"OP_CARRIER_FL_NUM\", outputCol=\"OP_CARRIER_FL_NUM_Index\")\n",
    "df_3 = OP_CARRIER_FL_NUM_indexer.fit(df_2).transform(df_2)\n",
    "\n",
    "onehotencoder_OP_CARRIER_FL_NUM_vector = OneHotEncoder(inputCol=\"OP_CARRIER_FL_NUM_Index\", outputCol=\"OP_CARRIER_FL_NUM_vec\")\n",
    "df_4 = onehotencoder_OP_CARRIER_FL_NUM_vector.fit(df_3).transform(df_3)\n",
    "\n",
    "ORIGIN_indexer = StringIndexer(inputCol=\"ORIGIN\", outputCol=\"ORIGIN_Index\")\n",
    "df_5 = ORIGIN_indexer.fit(df_4).transform(df_4)\n",
    "\n",
    "onehotencoder_ORIGIN_vector = OneHotEncoder(inputCol=\"ORIGIN_Index\", outputCol=\"ORIGIN_vec\")\n",
    "df_6 = onehotencoder_ORIGIN_vector.fit(df_5).transform(df_5)\n",
    "\n",
    "DEST_indexer = StringIndexer(inputCol=\"DEST\", outputCol=\"DEST_Index\")\n",
    "df_7 = DEST_indexer.fit(df_6).transform(df_6)\n",
    "\n",
    "onehotencoder_DEST_vector = OneHotEncoder(inputCol=\"DEST_Index\", outputCol=\"DEST_vec\")\n",
    "df_8 = onehotencoder_DEST_vector.fit(df_7).transform(df_7)\n",
    "df_8.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc31a225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|ARR_DELAY|\n",
      "+--------------------+---------+\n",
      "|(7924,[15,178,717...|      -27|\n",
      "|(7924,[15,689,717...|       -9|\n",
      "|(7924,[15,1073,71...|      -26|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['OP_CARRIER_vec', 'OP_CARRIER_FL_NUM_vec', 'ORIGIN_vec','DEST_vec',\n",
    "                                               'TAXI_OUT', 'TAXI_IN', 'CANCELLED','DIVERTED','CRS_ELAPSED_TIME',\n",
    "                                               'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'FL_YEAR', 'FL_MONTH',\n",
    "                                               'FL_DOM', 'FL_DOW'], outputCol = 'features')\n",
    "v_df = vectorAssembler.transform(df_8)\n",
    "v_df = v_df.select(['features', 'ARR_DELAY'])\n",
    "v_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b532e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = v_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "249b0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:47:28,258 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1240.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:50:52,958 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:50:53,886 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                        (0 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:51:32,960 WARN [Executor task launch worker for task 3.0 in stage 22.0 (TID 270)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_3 in memory! (computed 17.0 MiB so far)\n",
      "2022-12-15T03:51:32,988 WARN [Executor task launch worker for task 3.0 in stage 22.0 (TID 270)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_3 to disk instead.\n",
      "2022-12-15T03:51:33,044 WARN [Executor task launch worker for task 2.0 in stage 22.0 (TID 269)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_2 in memory! (computed 17.0 MiB so far)\n",
      "2022-12-15T03:51:33,045 WARN [Executor task launch worker for task 6.0 in stage 22.0 (TID 273)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_6 in memory! (computed 17.0 MiB so far)\n",
      "2022-12-15T03:51:33,046 WARN [Executor task launch worker for task 6.0 in stage 22.0 (TID 273)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_6 to disk instead.\n",
      "2022-12-15T03:51:33,047 WARN [Executor task launch worker for task 2.0 in stage 22.0 (TID 269)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_2 to disk instead.\n",
      "2022-12-15T03:51:35,843 WARN [Executor task launch worker for task 5.0 in stage 22.0 (TID 272)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_5 in memory! (computed 17.0 MiB so far)\n",
      "2022-12-15T03:51:35,844 WARN [Executor task launch worker for task 5.0 in stage 22.0 (TID 272)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_5 to disk instead.\n",
      "2022-12-15T03:51:36,267 WARN [Executor task launch worker for task 1.0 in stage 22.0 (TID 268)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_1 in memory! (computed 33.0 MiB so far)\n",
      "2022-12-15T03:51:36,267 WARN [Executor task launch worker for task 1.0 in stage 22.0 (TID 268)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_1 to disk instead.\n",
      "2022-12-15T03:51:36,571 WARN [Executor task launch worker for task 0.0 in stage 22.0 (TID 267)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_0 in memory! (computed 17.0 MiB so far)\n",
      "2022-12-15T03:51:36,571 WARN [Executor task launch worker for task 0.0 in stage 22.0 (TID 267)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_0 to disk instead.\n",
      "2022-12-15T03:51:36,577 WARN [Executor task launch worker for task 7.0 in stage 22.0 (TID 274)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_7 in memory! (computed 33.0 MiB so far)\n",
      "2022-12-15T03:51:36,578 WARN [Executor task launch worker for task 7.0 in stage 22.0 (TID 274)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_7 to disk instead.\n",
      "2022-12-15T03:51:36,596 WARN [Executor task launch worker for task 4.0 in stage 22.0 (TID 271)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_4 in memory! (computed 17.0 MiB so far)\n",
      "2022-12-15T03:51:36,596 WARN [Executor task launch worker for task 4.0 in stage 22.0 (TID 271)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_4 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                        (0 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:51:58,846 WARN [Executor task launch worker for task 1.0 in stage 22.0 (TID 268)] org.apache.spark.storage.BlockManager - Block rdd_75_1 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T03:51:59,342 ERROR [Executor task launch worker for task 1.0 in stage 22.0 (TID 268)] org.apache.spark.executor.Executor - Exception in task 1.0 in stage 22.0 (TID 268)\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.util.LinkedList.linkLast(LinkedList.java:142) ~[?:1.8.0_352]\n",
      "\tat java.util.LinkedList.add(LinkedList.java:338) ~[?:1.8.0_352]\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.append(BufferedRowIterator.java:73) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:163) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:783) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1527) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1525) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4641/1411228267.apply(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1525) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2155/2113215849.apply(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "2022-12-15T03:52:00,167 ERROR [Executor task launch worker for task 1.0 in stage 22.0 (TID 268)] org.apache.spark.util.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 22.0 (TID 268),5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.util.LinkedList.linkLast(LinkedList.java:142) ~[?:1.8.0_352]\n",
      "\tat java.util.LinkedList.add(LinkedList.java:338) ~[?:1.8.0_352]\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.append(BufferedRowIterator.java:73) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760) ~[spark-sql_2.12-3.3.1.jar:3.3.1]\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:163) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:783) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1527) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1525) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4641/1411228267.apply(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1525) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2155/2113215849.apply(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                        (0 + 9) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T03:52:03,714 WARN [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 22.0 (TID 268) (192.168.0.115 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.util.LinkedList.linkLast(LinkedList.java:142)\n",
      "\tat java.util.LinkedList.add(LinkedList.java:338)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.append(BufferedRowIterator.java:73)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:163)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:783)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1527)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1525)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4641/1411228267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1525)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2155/2113215849.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\n",
      "2022-12-15T03:52:03,719 ERROR [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 22.0 failed 1 times; aborting job\n",
      "2022-12-15T03:52:04,631 WARN [Executor task launch worker for task 6.0 in stage 22.0 (TID 273)] org.apache.spark.storage.BlockManager - Putting block rdd_75_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T03:52:04,632 WARN [Executor task launch worker for task 6.0 in stage 22.0 (TID 273)] org.apache.spark.storage.BlockManager - Block rdd_75_6 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T03:52:04,633 ERROR [Thread-4] org.apache.spark.ml.util.Instrumentation - org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 22.0 failed 1 times, most recent failure: Lost task 1.0 in stage 22.0 (TID 268) (192.168.0.115 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.util.LinkedList.linkLast(LinkedList.java:142)\n",
      "\tat java.util.LinkedList.add(LinkedList.java:338)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.append(BufferedRowIterator.java:73)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:163)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:783)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1527)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1525)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4641/1411228267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1525)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2155/2113215849.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainImpl(LinearRegression.scala:578)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:422)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.util.LinkedList.linkLast(LinkedList.java:142)\n",
      "\tat java.util.LinkedList.add(LinkedList.java:338)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.append(BufferedRowIterator.java:73)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:163)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n",
      "\tat org.apache.spark.storage.memory.PartiallyUnrolledIterator.next(MemoryStore.scala:783)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n",
      "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:177)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3(BlockManager.scala:1527)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$3$adapted(BlockManager.scala:1525)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$4641/1411228267.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:87)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1525)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2155/2113215849.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\n",
      "2022-12-15T03:52:04,636 WARN [Executor task launch worker for task 3.0 in stage 22.0 (TID 270)] org.apache.spark.storage.BlockManager - Putting block rdd_75_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T03:52:04,636 WARN [Executor task launch worker for task 3.0 in stage 22.0 (TID 270)] org.apache.spark.storage.BlockManager - Block rdd_75_3 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T03:52:04,640 WARN [Executor task launch worker for task 5.0 in stage 22.0 (TID 272)] org.apache.spark.storage.BlockManager - Putting block rdd_75_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T03:52:04,641 WARN [Executor task launch worker for task 5.0 in stage 22.0 (TID 272)] org.apache.spark.storage.BlockManager - Block rdd_75_5 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T03:52:04,647 WARN [Executor task launch worker for task 0.0 in stage 22.0 (TID 267)] org.apache.spark.storage.BlockManager - Putting block rdd_75_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T03:52:04,648 WARN [Executor task launch worker for task 0.0 in stage 22.0 (TID 267)] org.apache.spark.storage.BlockManager - Block rdd_75_0 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T03:52:04,651 WARN [Executor task launch worker for task 7.0 in stage 22.0 (TID 274)] org.apache.spark.storage.BlockManager - Putting block rdd_75_7 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T03:52:04,652 WARN [Executor task launch worker for task 7.0 in stage 22.0 (TID 274)] org.apache.spark.storage.BlockManager - Block rdd_75_7 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T03:52:04,663 WARN [Executor task launch worker for task 4.0 in stage 22.0 (TID 271)] org.apache.spark.storage.BlockManager - Putting block rdd_75_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T03:52:04,664 WARN [Executor task launch worker for task 4.0 in stage 22.0 (TID 271)] org.apache.spark.storage.BlockManager - Block rdd_75_4 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T03:52:04,666 WARN [Executor task launch worker for task 2.0 in stage 22.0 (TID 269)] org.apache.spark.storage.BlockManager - Putting block rdd_75_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T03:52:04,667 WARN [Executor task launch worker for task 2.0 in stage 22.0 (TID 269)] org.apache.spark.storage.BlockManager - Block rdd_75_2 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T03:52:05,304 WARN [Executor task launch worker for task 8.0 in stage 22.0 (TID 275)] org.apache.spark.storage.BlockManager - Putting block rdd_75_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T03:52:05,305 WARN [Executor task launch worker for task 8.0 in stage 22.0 (TID 275)] org.apache.spark.storage.BlockManager - Block rdd_75_8 could not be removed as it was not found on disk or in memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_6923/3802382033.py\", line 3, in <module>\n",
      "    lr_model = lr.fit(train_df)\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 383, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py\", line 380, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/anderl23/.local/lib/python3.10/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      2\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression(featuresCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARR_DELAY\u001b[39m\u001b[38;5;124m'\u001b[39m, maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, regParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, elasticNetParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficients: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lr_model\u001b[38;5;241m.\u001b[39mcoefficients))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2063\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2063\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2065\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py:541\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    535\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    536\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    538\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 541\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    542\u001b[0m }\n\u001b[1;32m    544\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='ARR_DELAY', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
