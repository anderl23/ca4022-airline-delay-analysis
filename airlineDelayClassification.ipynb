{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f80495-b6cd-48c7-865f-ed3f620db4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/anderl23/hadoop-3.3.4/spark-3.3.1/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/anderl23/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:03:58,229 WARN [main] org.apache.spark.util.Utils - Your hostname, DESKTOP-SHBG2MR resolves to a loopback address: 127.0.1.1; using 192.168.0.115 instead (on interface wifi0)\n",
      "2022-12-15T20:03:58,241 WARN [main] org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:03:59,369 WARN [Thread-4] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Load cleaned_airline_data.csv into Spark Dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ml-delay').getOrCreate()\n",
    "df = spark.read.csv('cleaned_airline_data_v5.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0756f2cd-9b20-4bc1-b4b1-0d3b090a5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEPT_TIME: double (nullable = true)\n",
      " |-- DEPT_TIME: double (nullable = true)\n",
      " |-- DEP_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: double (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- ARR_DELAY: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: integer (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- FL_YEAR: integer (nullable = true)\n",
      " |-- FL_MONTH: integer (nullable = true)\n",
      " |-- FL_DOM: integer (nullable = true)\n",
      " |-- FL_DOW: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17eccfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35841068"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment to calculate row count\n",
    "#Show number of rows in dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8112e156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FL_DATE</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <td>108</td>\n",
       "      <td>114</td>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN</th>\n",
       "      <td>LAX</td>\n",
       "      <td>LAX</td>\n",
       "      <td>JFK</td>\n",
       "      <td>PHL</td>\n",
       "      <td>LAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEST</th>\n",
       "      <td>IAD</td>\n",
       "      <td>IAD</td>\n",
       "      <td>SFO</td>\n",
       "      <td>LAX</td>\n",
       "      <td>PHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_DEPT_TIME</th>\n",
       "      <td>700.0</td>\n",
       "      <td>2205.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPT_TIME</th>\n",
       "      <td>700.0</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <td>708.0</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>1116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <td>1411.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>1828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_IN</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <td>1445.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_TIME</th>\n",
       "      <td>1418.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>1837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <td>-27</td>\n",
       "      <td>-9</td>\n",
       "      <td>-26</td>\n",
       "      <td>14</td>\n",
       "      <td>-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCELLED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIVERTED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <td>285</td>\n",
       "      <td>280</td>\n",
       "      <td>405</td>\n",
       "      <td>360</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <td>258</td>\n",
       "      <td>272</td>\n",
       "      <td>380</td>\n",
       "      <td>374</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIR_TIME</th>\n",
       "      <td>243</td>\n",
       "      <td>247</td>\n",
       "      <td>356</td>\n",
       "      <td>352</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTANCE</th>\n",
       "      <td>2288</td>\n",
       "      <td>2288</td>\n",
       "      <td>2586</td>\n",
       "      <td>2402</td>\n",
       "      <td>2402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_YEAR</th>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_MONTH</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_DOM</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_DOW</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0                    1  \\\n",
       "FL_DATE              2013-01-01 00:00:00  2013-01-01 00:00:00   \n",
       "OP_CARRIER                            VX                   VX   \n",
       "OP_CARRIER_FL_NUM                    108                  114   \n",
       "ORIGIN                               LAX                  LAX   \n",
       "DEST                                 IAD                  IAD   \n",
       "CRS_DEPT_TIME                      700.0               2205.0   \n",
       "DEPT_TIME                          700.0               2204.0   \n",
       "DEP_DELAY                              0                   -1   \n",
       "TAXI_OUT                               8                   12   \n",
       "WHEELS_OFF                         708.0               2216.0   \n",
       "WHEELS_ON                         1411.0                523.0   \n",
       "TAXI_IN                                7                   13   \n",
       "CRS_ARR_TIME                      1445.0                545.0   \n",
       "ARR_TIME                          1418.0                536.0   \n",
       "ARR_DELAY                            -27                   -9   \n",
       "CANCELLED                              0                    0   \n",
       "DIVERTED                               0                    0   \n",
       "CRS_ELAPSED_TIME                     285                  280   \n",
       "ACTUAL_ELAPSED_TIME                  258                  272   \n",
       "AIR_TIME                             243                  247   \n",
       "DISTANCE                            2288                 2288   \n",
       "FL_YEAR                             2013                 2013   \n",
       "FL_MONTH                               1                    1   \n",
       "FL_DOM                                 1                    1   \n",
       "FL_DOW                                 3                    3   \n",
       "\n",
       "                                       2                    3  \\\n",
       "FL_DATE              2013-01-01 00:00:00  2013-01-01 00:00:00   \n",
       "OP_CARRIER                            VX                   VX   \n",
       "OP_CARRIER_FL_NUM                     11                  121   \n",
       "ORIGIN                               JFK                  PHL   \n",
       "DEST                                 SFO                  LAX   \n",
       "CRS_DEPT_TIME                      730.0                700.0   \n",
       "DEPT_TIME                          729.0                700.0   \n",
       "DEP_DELAY                             -1                    0   \n",
       "TAXI_OUT                              18                   14   \n",
       "WHEELS_OFF                         747.0                714.0   \n",
       "WHEELS_ON                         1043.0               1006.0   \n",
       "TAXI_IN                                6                    8   \n",
       "CRS_ARR_TIME                      1115.0               1000.0   \n",
       "ARR_TIME                          1049.0               1014.0   \n",
       "ARR_DELAY                            -26                   14   \n",
       "CANCELLED                              0                    0   \n",
       "DIVERTED                               0                    0   \n",
       "CRS_ELAPSED_TIME                     405                  360   \n",
       "ACTUAL_ELAPSED_TIME                  380                  374   \n",
       "AIR_TIME                             356                  352   \n",
       "DISTANCE                            2586                 2402   \n",
       "FL_YEAR                             2013                 2013   \n",
       "FL_MONTH                               1                    1   \n",
       "FL_DOM                                 1                    1   \n",
       "FL_DOW                                 3                    3   \n",
       "\n",
       "                                       4  \n",
       "FL_DATE              2013-01-01 00:00:00  \n",
       "OP_CARRIER                            VX  \n",
       "OP_CARRIER_FL_NUM                    124  \n",
       "ORIGIN                               LAX  \n",
       "DEST                                 PHL  \n",
       "CRS_DEPT_TIME                     1100.0  \n",
       "DEPT_TIME                         1104.0  \n",
       "DEP_DELAY                              4  \n",
       "TAXI_OUT                              12  \n",
       "WHEELS_OFF                        1116.0  \n",
       "WHEELS_ON                         1828.0  \n",
       "TAXI_IN                                9  \n",
       "CRS_ARR_TIME                      1915.0  \n",
       "ARR_TIME                          1837.0  \n",
       "ARR_DELAY                            -38  \n",
       "CANCELLED                              0  \n",
       "DIVERTED                               0  \n",
       "CRS_ELAPSED_TIME                     315  \n",
       "ACTUAL_ELAPSED_TIME                  273  \n",
       "AIR_TIME                             252  \n",
       "DISTANCE                            2402  \n",
       "FL_YEAR                             2013  \n",
       "FL_MONTH                               1  \n",
       "FL_DOM                                 1  \n",
       "FL_DOW                                 3  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show first five rows in dataframe. (note this has been transposed in order to easily view)\n",
    "import pandas as pd\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea720d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:08:18,989 WARN [Thread-4] org.apache.spark.sql.catalyst.util.package - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "|            FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEPT_TIME|DEPT_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|FL_YEAR|FL_MONTH|FL_DOM|FL_DOW|OP_CARRIER_Index| OP_CARRIER_vec|OP_CARRIER_FL_NUM_Index|OP_CARRIER_FL_NUM_vec|ORIGIN_Index|      ORIGIN_vec|DEST_Index|        DEST_vec|\n",
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "|2013-01-01 00:00:00|        VX|              108|   LAX| IAD|        700.0|    700.0|        0|       8|     708.0|   1411.0|      7|      1445.0|  1418.0|      -27|        0|       0|             285|                258|     243|    2288|   2013|       1|     1|     3|            15.0|(19,[15],[1.0])|                  159.0|   (7154,[159],[1.0])|         4.0| (370,[4],[1.0])|      32.0|(369,[32],[1.0])|\n",
      "|2013-01-01 00:00:00|        VX|              114|   LAX| IAD|       2205.0|   2204.0|       -1|      12|    2216.0|    523.0|     13|       545.0|   536.0|       -9|        0|       0|             280|                272|     247|    2288|   2013|       1|     1|     3|            15.0|(19,[15],[1.0])|                  670.0|   (7154,[670],[1.0])|         4.0| (370,[4],[1.0])|      32.0|(369,[32],[1.0])|\n",
      "|2013-01-01 00:00:00|        VX|               11|   JFK| SFO|        730.0|    729.0|       -1|      18|     747.0|   1043.0|      6|      1115.0|  1049.0|      -26|        0|       0|             405|                380|     356|    2586|   2013|       1|     1|     3|            15.0|(19,[15],[1.0])|                 1054.0|  (7154,[1054],[1.0])|        18.0|(370,[18],[1.0])|       5.0| (369,[5],[1.0])|\n",
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "categoricalColumns = [\"OP_CARRIER\",\"OP_CARRIER_FL_NUM\",\"ORIGIN\",\"DEST\"]\n",
    "\n",
    "OP_CARRIER_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"OP_CARRIER_Index\")\n",
    "df_1 = OP_CARRIER_indexer.fit(df).transform(df)\n",
    "\n",
    "onehotencoder_OP_CARRIER_vector = OneHotEncoder(inputCol=\"OP_CARRIER_Index\", outputCol=\"OP_CARRIER_vec\")\n",
    "df_2 = onehotencoder_OP_CARRIER_vector.fit(df_1).transform(df_1)\n",
    "\n",
    "OP_CARRIER_FL_NUM_indexer = StringIndexer(inputCol=\"OP_CARRIER_FL_NUM\", outputCol=\"OP_CARRIER_FL_NUM_Index\")\n",
    "df_3 = OP_CARRIER_FL_NUM_indexer.fit(df_2).transform(df_2)\n",
    "\n",
    "onehotencoder_OP_CARRIER_FL_NUM_vector = OneHotEncoder(inputCol=\"OP_CARRIER_FL_NUM_Index\", outputCol=\"OP_CARRIER_FL_NUM_vec\")\n",
    "df_4 = onehotencoder_OP_CARRIER_FL_NUM_vector.fit(df_3).transform(df_3)\n",
    "\n",
    "ORIGIN_indexer = StringIndexer(inputCol=\"ORIGIN\", outputCol=\"ORIGIN_Index\")\n",
    "df_5 = ORIGIN_indexer.fit(df_4).transform(df_4)\n",
    "\n",
    "onehotencoder_ORIGIN_vector = OneHotEncoder(inputCol=\"ORIGIN_Index\", outputCol=\"ORIGIN_vec\")\n",
    "df_6 = onehotencoder_ORIGIN_vector.fit(df_5).transform(df_5)\n",
    "\n",
    "DEST_indexer = StringIndexer(inputCol=\"DEST\", outputCol=\"DEST_Index\")\n",
    "df_7 = DEST_indexer.fit(df_6).transform(df_6)\n",
    "\n",
    "onehotencoder_DEST_vector = OneHotEncoder(inputCol=\"DEST_Index\", outputCol=\"DEST_vec\")\n",
    "df_8 = onehotencoder_DEST_vector.fit(df_7).transform(df_7)\n",
    "df_8.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc31a225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|ARR_DELAY|\n",
      "+--------------------+---------+\n",
      "|(7924,[15,178,717...|      -27|\n",
      "|(7924,[15,689,717...|       -9|\n",
      "|(7924,[15,1073,71...|      -26|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['OP_CARRIER_vec', 'OP_CARRIER_FL_NUM_vec', 'ORIGIN_vec','DEST_vec',\n",
    "                                               'TAXI_OUT', 'TAXI_IN', 'CANCELLED','DIVERTED','CRS_ELAPSED_TIME',\n",
    "                                               'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'FL_YEAR', 'FL_MONTH',\n",
    "                                               'FL_DOM', 'FL_DOW'], outputCol = 'features')\n",
    "v_df = vectorAssembler.transform(df_8)\n",
    "v_df = v_df.select(['features', 'ARR_DELAY'])\n",
    "v_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b532e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = v_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249b0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:08:21,573 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1240.9 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:14:41,644 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:14:43,497 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                        (0 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:16:27,926 WARN [Executor task launch worker for task 1.0 in stage 22.0 (TID 268)] dev.ludovic.netlib.InstanceBuilder$NativeBLAS - Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2022-12-15T20:16:28,001 WARN [Executor task launch worker for task 1.0 in stage 22.0 (TID 268)] dev.ludovic.netlib.InstanceBuilder$NativeBLAS - Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=================>                                      (11 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:20:15,961 WARN [Executor task launch worker for task 14.0 in stage 22.0 (TID 281)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_14 in memory! (computed 113.0 MiB so far)\n",
      "2022-12-15T20:20:16,467 WARN [Executor task launch worker for task 14.0 in stage 22.0 (TID 281)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_14 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:======================================>                 (25 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:24:12,305 WARN [Executor task launch worker for task 24.0 in stage 22.0 (TID 291)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 113.0 MiB so far)\n",
      "2022-12-15T20:24:12,319 WARN [Executor task launch worker for task 24.0 in stage 22.0 (TID 291)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_24 to disk instead.\n",
      "2022-12-15T20:24:20,742 WARN [Executor task launch worker for task 24.0 in stage 22.0 (TID 291)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 113.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:========================================>               (26 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:24:30,906 WARN [Executor task launch worker for task 25.0 in stage 22.0 (TID 292)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:24:30,907 WARN [Executor task launch worker for task 25.0 in stage 22.0 (TID 292)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_25 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:========================================>               (26 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:25:36,307 WARN [Executor task launch worker for task 25.0 in stage 22.0 (TID 292)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 113.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:==========================================>             (27 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:25:52,093 WARN [Executor task launch worker for task 30.0 in stage 22.0 (TID 297)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_30 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:25:52,096 WARN [Executor task launch worker for task 30.0 in stage 22.0 (TID 297)] org.apache.spark.storage.BlockManager - Persisting block rdd_75_30 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:26:28,355 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:26:29,819 WARN [Thread-4] com.github.fommil.netlib.BLAS - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2022-12-15T20:26:29,859 WARN [Thread-4] com.github.fommil.netlib.BLAS - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2022-12-15T20:26:30,271 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:27:41,672 WARN [Executor task launch worker for task 25.0 in stage 24.0 (TID 334)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:27:41,672 WARN [Executor task launch worker for task 24.0 in stage 24.0 (TID 333)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:27:52,094 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:27:53,094 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:================================>                       (21 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:28:15,556 WARN [Executor task launch worker for task 24.0 in stage 26.0 (TID 375)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:28:15,661 WARN [Executor task launch worker for task 25.0 in stage 26.0 (TID 376)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:28:24,511 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:28:25,415 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:================================>                       (21 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:28:44,645 WARN [Executor task launch worker for task 24.0 in stage 28.0 (TID 417)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:===================================>                    (23 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:28:45,518 WARN [Executor task launch worker for task 25.0 in stage 28.0 (TID 418)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:28:54,857 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:28:55,657 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===============================>                        (20 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:29:16,320 WARN [Executor task launch worker for task 24.0 in stage 30.0 (TID 459)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:29:19,602 WARN [Executor task launch worker for task 25.0 in stage 30.0 (TID 460)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 113.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:29:24,668 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:29:25,339 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:==================================>                     (22 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:29:45,405 WARN [Executor task launch worker for task 24.0 in stage 32.0 (TID 501)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:29:45,790 WARN [Executor task launch worker for task 25.0 in stage 32.0 (TID 502)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:29:53,465 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:29:56,795 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:30:17,370 WARN [Executor task launch worker for task 24.0 in stage 34.0 (TID 543)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:30:17,509 WARN [Executor task launch worker for task 25.0 in stage 34.0 (TID 544)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:====================================================>   (34 + 2) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:30:26,691 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:30:27,636 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:30:49,367 WARN [Executor task launch worker for task 25.0 in stage 36.0 (TID 586)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:30:49,375 WARN [Executor task launch worker for task 24.0 in stage 36.0 (TID 585)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:30:56,082 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:30:56,995 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:31:20,023 WARN [Executor task launch worker for task 25.0 in stage 38.0 (TID 628)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:31:20,062 WARN [Executor task launch worker for task 24.0 in stage 38.0 (TID 627)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:31:28,219 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:31:30,792 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:===============================>                        (20 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:31:51,806 WARN [Executor task launch worker for task 24.0 in stage 40.0 (TID 669)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:31:52,262 WARN [Executor task launch worker for task 25.0 in stage 40.0 (TID 670)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:32:02,509 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:32:03,337 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:================================>                       (21 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:32:25,944 WARN [Executor task launch worker for task 25.0 in stage 42.0 (TID 712)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:32:26,066 WARN [Executor task launch worker for task 24.0 in stage 42.0 (TID 711)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:32:33,019 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:32:33,900 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:================================>                       (21 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:32:56,142 WARN [Executor task launch worker for task 25.0 in stage 44.0 (TID 754)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:32:56,209 WARN [Executor task launch worker for task 24.0 in stage 44.0 (TID 753)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:33:03,778 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:33:04,563 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:================================>                       (21 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:33:31,304 WARN [Executor task launch worker for task 24.0 in stage 46.0 (TID 795)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:33:31,783 WARN [Executor task launch worker for task 25.0 in stage 46.0 (TID 796)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:33:39,354 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:33:42,480 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:================================================>       (31 + 5) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:34:30,967 WARN [Executor task launch worker for task 24.0 in stage 48.0 (TID 837)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:34:30,922 WARN [Executor task launch worker for task 25.0 in stage 48.0 (TID 838)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:====================================================>   (34 + 2) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:34:33,299 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:34:34,429 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:==================================>                     (22 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:34:55,325 WARN [Executor task launch worker for task 25.0 in stage 50.0 (TID 880)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:34:55,466 WARN [Executor task launch worker for task 24.0 in stage 50.0 (TID 879)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:35:03,432 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:35:04,315 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:===================================>                    (23 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:35:27,649 WARN [Executor task launch worker for task 24.0 in stage 52.0 (TID 921)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:35:28,177 WARN [Executor task launch worker for task 25.0 in stage 52.0 (TID 922)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:========================================================(36 + 0) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:35:35,323 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:35:36,349 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:===================================>                    (23 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:35:58,717 WARN [Executor task launch worker for task 24.0 in stage 54.0 (TID 963)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:35:58,987 WARN [Executor task launch worker for task 25.0 in stage 54.0 (TID 964)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:36:06,505 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:36:07,572 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:=============================>                          (19 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:36:28,846 WARN [Executor task launch worker for task 24.0 in stage 56.0 (TID 1005)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:===================================>                    (23 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:36:29,379 WARN [Executor task launch worker for task 25.0 in stage 56.0 (TID 1006)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:36:37,718 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:36:39,356 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:37:02,983 WARN [Executor task launch worker for task 24.0 in stage 58.0 (TID 1047)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:======================================>                 (25 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:37:06,804 WARN [Executor task launch worker for task 25.0 in stage 58.0 (TID 1048)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 113.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:========================================================(36 + 0) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:37:12,716 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:37:13,604 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:37:36,660 WARN [Executor task launch worker for task 24.0 in stage 60.0 (TID 1089)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:37:37,260 WARN [Executor task launch worker for task 25.0 in stage 60.0 (TID 1090)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:37:45,670 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:37:46,572 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1242.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=====================================>                  (24 + 8) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:38:08,636 WARN [Executor task launch worker for task 25.0 in stage 62.0 (TID 1132)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_25 in memory! (computed 65.0 MiB so far)\n",
      "2022-12-15T20:38:08,728 WARN [Executor task launch worker for task 24.0 in stage 62.0 (TID 1131)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_75_24 in memory! (computed 65.0 MiB so far)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:===================================================>    (33 + 3) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:38:18,742 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:38:22,270 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1246.4 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:========================================================(36 + 0) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:44:49,543 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1247.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (7924,[0,1,2,3,5,6,7,8,9,10,11,12,14,15,16,17,7174,7175,7182,7183,7184,7185,7187,7188,7189,7190,7191,7193,7194,7195,7196,7200,7203,7543,7546,7547,7548,7549,7552,7554,7555,7558,7559,7560,7577,7750,7912,7913,7916,7917,7918,7919,7920,7921],[6.272537433773334,-3.705960270638793,-0.4593746255635295,-0.2152964124597678,0.7986381437778339,5.25879845315017,0.008884025169823872,-2.6176271432260556,-2.65277195151177,2.604362922884076,4.030152423498098,-2.2187722021360616,0.004947141666011967,0.017781861265594324,-2.0311774623319376,1.124235306514084,1.3100636487296098,2.471272063626226,-0.01066141753895997,-2.219679273646862,-1.5494986438191842,-1.681873550961087,-3.8795155199415627,-0.6646018705899781,-2.840406411261809,-8.958707122604292,-9.839252782508273,0.006298392450582942,-3.3521006981853585,-3.4911263856895944,-0.027687428933329823,1.244283628428662,1.2241527688836362,-0.3493155882209044,-0.12182269510896811,-2.3991354440349455,3.7776832469482406,-0.6020933725056239,-2.380437789706886,-0.16573986210282415,-0.005641813291694282,1.9428775153211721,-0.013307010365760683,0.06393580663776202,0.012004007693373737,6.296727992068942,1.1233983278931066,0.8401201393896899,-0.22488535538388282,0.10045129797344003,0.1279744438275789,-0.002844533986117927,-0.7598872334955948,-0.06734505247030088])\n",
      "Intercept: 1518.220604715031\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='ARR_DELAY', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029e316d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 40.077081\n",
      "r2: 0.095762\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdff281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:48:14,206 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1249.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|         ARR_DELAY|\n",
      "+-------+------------------+\n",
      "|  count|          25094263|\n",
      "|   mean| 5.127828141436152|\n",
      "| stddev|42.145850612557226|\n",
      "|    min|              -238|\n",
      "|    max|              2692|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ebfa43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:53:51,521 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1239.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+--------------------+\n",
      "|        prediction|ARR_DELAY|            features|\n",
      "+------------------+---------+--------------------+\n",
      "|1.0506625996729326|      -11|(7924,[0,19,7177,...|\n",
      "| 3.241952133061659|      -14|(7924,[0,19,7177,...|\n",
      "| 2.974165315333721|       -9|(7924,[0,19,7177,...|\n",
      "|  8.87288815147258|      -12|(7924,[0,19,7177,...|\n",
      "|-5.683861224159955|      -11|(7924,[0,19,7179,...|\n",
      "+------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "2022-12-15T20:54:10,819 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1247.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:59:50,849 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1248.1 KiB\n",
      "R Squared (R2) on test data = 0.0953659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"ARR_DELAY\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"ARR_DELAY\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63ad8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T20:59:59,208 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1246.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:========================================================(36 + 0) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T21:05:46,709 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1247.9 KiB\n",
      "Root Mean Squared Error (RMSE) on test data = 40.1314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_result = lr_model.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79194c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 10\n",
      "objectiveHistory: [0.49999998007511887, 0.48986914389878855, 0.4669502038466114, 0.46698181446507875, 0.46523675455697294, 0.4651261619426882, 0.4647588589230092, 0.46445430639189894, 0.46164934972839905, 0.46161606907324493, 0.4614816236660369]\n",
      "2022-12-15T21:05:51,482 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1243.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-16.922398515348732|\n",
      "|-13.310532927567465|\n",
      "|-12.617082699802268|\n",
      "|-15.227951231503312|\n",
      "|-15.753894749321262|\n",
      "| -8.959789450942708|\n",
      "|-13.128786630106788|\n",
      "|-14.334315751009171|\n",
      "|-15.498361851406116|\n",
      "|-15.525230321565232|\n",
      "|-14.379504726132382|\n",
      "|-16.269314016357384|\n",
      "|-16.426440683001374|\n",
      "| -10.02714210017939|\n",
      "|  -17.8180517459964|\n",
      "|-14.366167382387857|\n",
      "| -8.446509277375071|\n",
      "| -5.236779003370884|\n",
      "| -11.52103642138104|\n",
      "| 57.594223817465036|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0db7bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T21:06:41,208 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1239.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+--------------------+\n",
      "|         prediction|ARR_DELAY|            features|\n",
      "+-------------------+---------+--------------------+\n",
      "| 1.0506625996729326|      -11|(7924,[0,19,7177,...|\n",
      "|  3.241952133061659|      -14|(7924,[0,19,7177,...|\n",
      "|  2.974165315333721|       -9|(7924,[0,19,7177,...|\n",
      "|   8.87288815147258|      -12|(7924,[0,19,7177,...|\n",
      "| -5.683861224159955|      -11|(7924,[0,19,7179,...|\n",
      "| -3.369258685214163|       -6|(7924,[0,19,7179,...|\n",
      "| -3.181309525724373|        8|(7924,[0,19,7179,...|\n",
      "| -4.914501937503246|       -1|(7924,[0,19,7179,...|\n",
      "|  1.252993091124381|       11|(7924,[0,19,7179,...|\n",
      "|  4.249396204696495|        2|(7924,[0,19,7179,...|\n",
      "| 2.0468597035478524|      -15|(7924,[0,19,7179,...|\n",
      "| 0.7031737229008286|      -13|(7924,[0,19,7179,...|\n",
      "| 2.8418418223557183|        7|(7924,[0,19,7179,...|\n",
      "|  3.769920653950976|        2|(7924,[0,19,7179,...|\n",
      "| -4.320928385468733|      -16|(7924,[0,19,7181,...|\n",
      "|-1.9549500505970627|       -6|(7924,[0,19,7181,...|\n",
      "| 0.6383385196913878|        4|(7924,[0,19,7181,...|\n",
      "| -2.047917935218493|      -18|(7924,[0,19,7181,...|\n",
      "|   5.59983643762007|       16|(7924,[0,19,7181,...|\n",
      "| 12.616257614921096|        9|(7924,[0,19,7185,...|\n",
      "+-------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = lr_model.transform(test_df)\n",
    "predictions.select(\"prediction\",\"ARR_DELAY\",\"features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdd3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
