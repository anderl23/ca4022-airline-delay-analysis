{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f80495-b6cd-48c7-865f-ed3f620db4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/anderl23/hadoop-3.3.4/spark-3.3.1/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/anderl23/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-14T22:27:01,540 WARN [main] org.apache.spark.util.Utils - Your hostname, DESKTOP-SHBG2MR resolves to a loopback address: 127.0.1.1; using 192.168.0.115 instead (on interface wifi0)\n",
      "2022-12-14T22:27:01,627 WARN [main] org.apache.spark.util.Utils - Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-14T22:27:04,461 WARN [Thread-4] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anderl23/.local/lib/python3.10/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-14T22:30:07,622 WARN [Thread-4] org.apache.spark.sql.catalyst.util.package - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_c0=datetime.datetime(2013, 1, 1, 0, 0), _c1='VX', _c2=108, _c3='LAX', _c4='IAD', _c5=700.0, _c6=700.0, _c7=0, _c8=8, _c9=708.0, _c10=1411.0, _c11=7, _c12=1445.0, _c13=1418.0, _c14=-27, _c15=0, _c16=None, _c17=0, _c18=285, _c19=258, _c20=243, _c21=2288, _c22=None, _c23=None, _c24=None, _c25=None, _c26=None)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load cleaned_airline_data.csv into Spark Dataframe\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.appName('ml-delay').getOrCreate()\n",
    "# df = spark.read.csv('cleaned_airline_data.csv', header = False, inferSchema = True)\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc= SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='false', inferschema='true').load('cleaned_airline_data.csv')\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0756f2cd-9b20-4bc1-b4b1-0d3b090a5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEPT_TIME: double (nullable = true)\n",
      " |-- DEPT_TIME: double (nullable = true)\n",
      " |-- DEP_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- CRS_ARR_TIME: double (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- ARR_DELAY: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLEATION_CODE: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: integer (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- CARRIER_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      " |-- NAS_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rename columns\n",
    "Data_list = [\"FL_DATE\",\"OP_CARRIER\",\"OP_CARRIER_FL_NUM\",\"ORIGIN\",\"DEST\",\"CRS_DEPT_TIME\",\"DEPT_TIME\",\"DEP_DELAY\",\n",
    "             \"TAXI_OUT\",\"WHEELS_OFF\",\"WHEELS_ON\",\"TAXI_IN\",\"CRS_ARR_TIME\",\"ARR_TIME\",\"ARR_DELAY\",\"CANCELLED\",\n",
    "             \"CANCELLEATION_CODE\",\"DIVERTED\",\"CRS_ELAPSED_TIME\",\"ACTUAL_ELAPSED_TIME\",\"AIR_TIME\",\"DISTANCE\",\n",
    "             \"CARRIER_DELAY\",\"WEATHER_DELAY\",\"NAS_DELAY\",\"SECURITY_DELAY\",\"LATE_AIRCRAFT_DELAY\"]\n",
    "df = df.toDF(*Data_list)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17eccfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68979001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show number of rows in dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8112e156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FL_DATE</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <td>108</td>\n",
       "      <td>114</td>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN</th>\n",
       "      <td>LAX</td>\n",
       "      <td>LAX</td>\n",
       "      <td>JFK</td>\n",
       "      <td>PHL</td>\n",
       "      <td>LAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEST</th>\n",
       "      <td>IAD</td>\n",
       "      <td>IAD</td>\n",
       "      <td>SFO</td>\n",
       "      <td>LAX</td>\n",
       "      <td>PHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_DEPT_TIME</th>\n",
       "      <td>700.0</td>\n",
       "      <td>2205.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPT_TIME</th>\n",
       "      <td>700.0</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <td>708.0</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>1116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <td>1411.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>1828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_IN</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <td>1445.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_TIME</th>\n",
       "      <td>1418.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>1837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <td>-27</td>\n",
       "      <td>-9</td>\n",
       "      <td>-26</td>\n",
       "      <td>14</td>\n",
       "      <td>-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCELLED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCELLEATION_CODE</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIVERTED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <td>285</td>\n",
       "      <td>280</td>\n",
       "      <td>405</td>\n",
       "      <td>360</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <td>258</td>\n",
       "      <td>272</td>\n",
       "      <td>380</td>\n",
       "      <td>374</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIR_TIME</th>\n",
       "      <td>243</td>\n",
       "      <td>247</td>\n",
       "      <td>356</td>\n",
       "      <td>352</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTANCE</th>\n",
       "      <td>2288</td>\n",
       "      <td>2288</td>\n",
       "      <td>2586</td>\n",
       "      <td>2402</td>\n",
       "      <td>2402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0                    1  \\\n",
       "FL_DATE              2013-01-01 00:00:00  2013-01-01 00:00:00   \n",
       "OP_CARRIER                            VX                   VX   \n",
       "OP_CARRIER_FL_NUM                    108                  114   \n",
       "ORIGIN                               LAX                  LAX   \n",
       "DEST                                 IAD                  IAD   \n",
       "CRS_DEPT_TIME                      700.0               2205.0   \n",
       "DEPT_TIME                          700.0               2204.0   \n",
       "DEP_DELAY                              0                   -1   \n",
       "TAXI_OUT                               8                   12   \n",
       "WHEELS_OFF                         708.0               2216.0   \n",
       "WHEELS_ON                         1411.0                523.0   \n",
       "TAXI_IN                                7                   13   \n",
       "CRS_ARR_TIME                      1445.0                545.0   \n",
       "ARR_TIME                          1418.0                536.0   \n",
       "ARR_DELAY                            -27                   -9   \n",
       "CANCELLED                              0                    0   \n",
       "CANCELLEATION_CODE                  None                 None   \n",
       "DIVERTED                               0                    0   \n",
       "CRS_ELAPSED_TIME                     285                  280   \n",
       "ACTUAL_ELAPSED_TIME                  258                  272   \n",
       "AIR_TIME                             243                  247   \n",
       "DISTANCE                            2288                 2288   \n",
       "CARRIER_DELAY                       None                 None   \n",
       "WEATHER_DELAY                       None                 None   \n",
       "NAS_DELAY                           None                 None   \n",
       "SECURITY_DELAY                      None                 None   \n",
       "LATE_AIRCRAFT_DELAY                 None                 None   \n",
       "\n",
       "                                       2                    3  \\\n",
       "FL_DATE              2013-01-01 00:00:00  2013-01-01 00:00:00   \n",
       "OP_CARRIER                            VX                   VX   \n",
       "OP_CARRIER_FL_NUM                     11                  121   \n",
       "ORIGIN                               JFK                  PHL   \n",
       "DEST                                 SFO                  LAX   \n",
       "CRS_DEPT_TIME                      730.0                700.0   \n",
       "DEPT_TIME                          729.0                700.0   \n",
       "DEP_DELAY                             -1                    0   \n",
       "TAXI_OUT                              18                   14   \n",
       "WHEELS_OFF                         747.0                714.0   \n",
       "WHEELS_ON                         1043.0               1006.0   \n",
       "TAXI_IN                                6                    8   \n",
       "CRS_ARR_TIME                      1115.0               1000.0   \n",
       "ARR_TIME                          1049.0               1014.0   \n",
       "ARR_DELAY                            -26                   14   \n",
       "CANCELLED                              0                    0   \n",
       "CANCELLEATION_CODE                  None                 None   \n",
       "DIVERTED                               0                    0   \n",
       "CRS_ELAPSED_TIME                     405                  360   \n",
       "ACTUAL_ELAPSED_TIME                  380                  374   \n",
       "AIR_TIME                             356                  352   \n",
       "DISTANCE                            2586                 2402   \n",
       "CARRIER_DELAY                       None                 None   \n",
       "WEATHER_DELAY                       None                 None   \n",
       "NAS_DELAY                           None                 None   \n",
       "SECURITY_DELAY                      None                 None   \n",
       "LATE_AIRCRAFT_DELAY                 None                 None   \n",
       "\n",
       "                                       4  \n",
       "FL_DATE              2013-01-01 00:00:00  \n",
       "OP_CARRIER                            VX  \n",
       "OP_CARRIER_FL_NUM                    124  \n",
       "ORIGIN                               LAX  \n",
       "DEST                                 PHL  \n",
       "CRS_DEPT_TIME                     1100.0  \n",
       "DEPT_TIME                         1104.0  \n",
       "DEP_DELAY                              4  \n",
       "TAXI_OUT                              12  \n",
       "WHEELS_OFF                        1116.0  \n",
       "WHEELS_ON                         1828.0  \n",
       "TAXI_IN                                9  \n",
       "CRS_ARR_TIME                      1915.0  \n",
       "ARR_TIME                          1837.0  \n",
       "ARR_DELAY                            -38  \n",
       "CANCELLED                              0  \n",
       "CANCELLEATION_CODE                  None  \n",
       "DIVERTED                               0  \n",
       "CRS_ELAPSED_TIME                     315  \n",
       "ACTUAL_ELAPSED_TIME                  273  \n",
       "AIR_TIME                             252  \n",
       "DISTANCE                            2402  \n",
       "CARRIER_DELAY                       None  \n",
       "WEATHER_DELAY                       None  \n",
       "NAS_DELAY                           None  \n",
       "SECURITY_DELAY                      None  \n",
       "LATE_AIRCRAFT_DELAY                 None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show first five rows in dataframe. (note this has been transposed in order to easily view)\n",
    "import pandas as pd\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0eef9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FL_DATE</th>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "      <td>2013-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "      <td>VX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <td>108</td>\n",
       "      <td>114</td>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN</th>\n",
       "      <td>LAX</td>\n",
       "      <td>LAX</td>\n",
       "      <td>JFK</td>\n",
       "      <td>PHL</td>\n",
       "      <td>LAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEST</th>\n",
       "      <td>IAD</td>\n",
       "      <td>IAD</td>\n",
       "      <td>SFO</td>\n",
       "      <td>LAX</td>\n",
       "      <td>PHL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_DEPT_TIME</th>\n",
       "      <td>700.0</td>\n",
       "      <td>2205.0</td>\n",
       "      <td>730.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPT_TIME</th>\n",
       "      <td>700.0</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>1104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <td>708.0</td>\n",
       "      <td>2216.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>714.0</td>\n",
       "      <td>1116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <td>1411.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>1043.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>1828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_IN</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <td>1445.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_TIME</th>\n",
       "      <td>1418.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>1049.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>1837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <td>-27</td>\n",
       "      <td>-9</td>\n",
       "      <td>-26</td>\n",
       "      <td>14</td>\n",
       "      <td>-38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCELLED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCELLEATION_CODE</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIVERTED</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <td>285</td>\n",
       "      <td>280</td>\n",
       "      <td>405</td>\n",
       "      <td>360</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <td>258</td>\n",
       "      <td>272</td>\n",
       "      <td>380</td>\n",
       "      <td>374</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIR_TIME</th>\n",
       "      <td>243</td>\n",
       "      <td>247</td>\n",
       "      <td>356</td>\n",
       "      <td>352</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTANCE</th>\n",
       "      <td>2288</td>\n",
       "      <td>2288</td>\n",
       "      <td>2586</td>\n",
       "      <td>2402</td>\n",
       "      <td>2402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_YEAR</th>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_MONTH</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_DOM</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_DOW</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0                    1  \\\n",
       "FL_DATE              2013-01-01 00:00:00  2013-01-01 00:00:00   \n",
       "OP_CARRIER                            VX                   VX   \n",
       "OP_CARRIER_FL_NUM                    108                  114   \n",
       "ORIGIN                               LAX                  LAX   \n",
       "DEST                                 IAD                  IAD   \n",
       "CRS_DEPT_TIME                      700.0               2205.0   \n",
       "DEPT_TIME                          700.0               2204.0   \n",
       "DEP_DELAY                              0                   -1   \n",
       "TAXI_OUT                               8                   12   \n",
       "WHEELS_OFF                         708.0               2216.0   \n",
       "WHEELS_ON                         1411.0                523.0   \n",
       "TAXI_IN                                7                   13   \n",
       "CRS_ARR_TIME                      1445.0                545.0   \n",
       "ARR_TIME                          1418.0                536.0   \n",
       "ARR_DELAY                            -27                   -9   \n",
       "CANCELLED                              0                    0   \n",
       "CANCELLEATION_CODE                  None                 None   \n",
       "DIVERTED                               0                    0   \n",
       "CRS_ELAPSED_TIME                     285                  280   \n",
       "ACTUAL_ELAPSED_TIME                  258                  272   \n",
       "AIR_TIME                             243                  247   \n",
       "DISTANCE                            2288                 2288   \n",
       "CARRIER_DELAY                       None                 None   \n",
       "WEATHER_DELAY                       None                 None   \n",
       "NAS_DELAY                           None                 None   \n",
       "SECURITY_DELAY                      None                 None   \n",
       "LATE_AIRCRAFT_DELAY                 None                 None   \n",
       "FL_YEAR                             2013                 2013   \n",
       "FL_MONTH                               1                    1   \n",
       "FL_DOM                                 1                    1   \n",
       "FL_DOW                                 3                    3   \n",
       "\n",
       "                                       2                    3  \\\n",
       "FL_DATE              2013-01-01 00:00:00  2013-01-01 00:00:00   \n",
       "OP_CARRIER                            VX                   VX   \n",
       "OP_CARRIER_FL_NUM                     11                  121   \n",
       "ORIGIN                               JFK                  PHL   \n",
       "DEST                                 SFO                  LAX   \n",
       "CRS_DEPT_TIME                      730.0                700.0   \n",
       "DEPT_TIME                          729.0                700.0   \n",
       "DEP_DELAY                             -1                    0   \n",
       "TAXI_OUT                              18                   14   \n",
       "WHEELS_OFF                         747.0                714.0   \n",
       "WHEELS_ON                         1043.0               1006.0   \n",
       "TAXI_IN                                6                    8   \n",
       "CRS_ARR_TIME                      1115.0               1000.0   \n",
       "ARR_TIME                          1049.0               1014.0   \n",
       "ARR_DELAY                            -26                   14   \n",
       "CANCELLED                              0                    0   \n",
       "CANCELLEATION_CODE                  None                 None   \n",
       "DIVERTED                               0                    0   \n",
       "CRS_ELAPSED_TIME                     405                  360   \n",
       "ACTUAL_ELAPSED_TIME                  380                  374   \n",
       "AIR_TIME                             356                  352   \n",
       "DISTANCE                            2586                 2402   \n",
       "CARRIER_DELAY                       None                 None   \n",
       "WEATHER_DELAY                       None                 None   \n",
       "NAS_DELAY                           None                 None   \n",
       "SECURITY_DELAY                      None                 None   \n",
       "LATE_AIRCRAFT_DELAY                 None                 None   \n",
       "FL_YEAR                             2013                 2013   \n",
       "FL_MONTH                               1                    1   \n",
       "FL_DOM                                 1                    1   \n",
       "FL_DOW                                 3                    3   \n",
       "\n",
       "                                       4  \n",
       "FL_DATE              2013-01-01 00:00:00  \n",
       "OP_CARRIER                            VX  \n",
       "OP_CARRIER_FL_NUM                    124  \n",
       "ORIGIN                               LAX  \n",
       "DEST                                 PHL  \n",
       "CRS_DEPT_TIME                     1100.0  \n",
       "DEPT_TIME                         1104.0  \n",
       "DEP_DELAY                              4  \n",
       "TAXI_OUT                              12  \n",
       "WHEELS_OFF                        1116.0  \n",
       "WHEELS_ON                         1828.0  \n",
       "TAXI_IN                                9  \n",
       "CRS_ARR_TIME                      1915.0  \n",
       "ARR_TIME                          1837.0  \n",
       "ARR_DELAY                            -38  \n",
       "CANCELLED                              0  \n",
       "CANCELLEATION_CODE                  None  \n",
       "DIVERTED                               0  \n",
       "CRS_ELAPSED_TIME                     315  \n",
       "ACTUAL_ELAPSED_TIME                  273  \n",
       "AIR_TIME                             252  \n",
       "DISTANCE                            2402  \n",
       "CARRIER_DELAY                       None  \n",
       "WEATHER_DELAY                       None  \n",
       "NAS_DELAY                           None  \n",
       "SECURITY_DELAY                      None  \n",
       "LATE_AIRCRAFT_DELAY                 None  \n",
       "FL_YEAR                             2013  \n",
       "FL_MONTH                               1  \n",
       "FL_DOM                                 1  \n",
       "FL_DOW                                 3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import year\n",
    "from pyspark.sql.functions import month\n",
    "from pyspark.sql.functions import dayofmonth\n",
    "from pyspark.sql.functions import dayofweek\n",
    "\n",
    "df = df.withColumn('FL_YEAR',year(df.FL_DATE))\n",
    "df = df.withColumn('FL_MONTH',month(df.FL_DATE))\n",
    "df = df.withColumn('FL_DOM',dayofmonth(df.FL_DATE))\n",
    "df = df.withColumn('FL_DOW',dayofweek(df.FL_DATE))\n",
    "\n",
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105deca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <td>68979001</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9E</td>\n",
       "      <td>YX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <td>68979001</td>\n",
       "      <td>2327.889195336418</td>\n",
       "      <td>1879.0626021936075</td>\n",
       "      <td>0</td>\n",
       "      <td>9855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORIGIN</th>\n",
       "      <td>68979001</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ABE</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEST</th>\n",
       "      <td>68979001</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ABE</td>\n",
       "      <td>YUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_DEPT_TIME</th>\n",
       "      <td>68848914</td>\n",
       "      <td>1327.9867342279356</td>\n",
       "      <td>478.4238633024651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPT_TIME</th>\n",
       "      <td>67913168</td>\n",
       "      <td>1191.1727909085319</td>\n",
       "      <td>616.3955895624565</td>\n",
       "      <td>-82.0</td>\n",
       "      <td>2710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <td>67904349</td>\n",
       "      <td>9.940476875788914</td>\n",
       "      <td>35.335181803239514</td>\n",
       "      <td>-251</td>\n",
       "      <td>2755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <td>67881123</td>\n",
       "      <td>160.10510623107987</td>\n",
       "      <td>447.82081350655903</td>\n",
       "      <td>0</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <td>67877458</td>\n",
       "      <td>1367.072854879156</td>\n",
       "      <td>495.35591549375897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <td>67844338</td>\n",
       "      <td>1316.3517606141281</td>\n",
       "      <td>662.976840709899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAXI_IN</th>\n",
       "      <td>67844340</td>\n",
       "      <td>163.4307853831285</td>\n",
       "      <td>484.5105195689285</td>\n",
       "      <td>0</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <td>68825194</td>\n",
       "      <td>1337.8248740134318</td>\n",
       "      <td>655.5450636017595</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>2695.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_TIME</th>\n",
       "      <td>67828181</td>\n",
       "      <td>1332.7721060218319</td>\n",
       "      <td>645.2998457732903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <td>67857650</td>\n",
       "      <td>91.74694898806545</td>\n",
       "      <td>318.563018352869</td>\n",
       "      <td>-411</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCELLED</th>\n",
       "      <td>62946217</td>\n",
       "      <td>0.4817790559200722</td>\n",
       "      <td>10.299515858630517</td>\n",
       "      <td>0</td>\n",
       "      <td>2695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CANCELLEATION_CODE</th>\n",
       "      <td>1389253</td>\n",
       "      <td>3.8024038817983477</td>\n",
       "      <td>32.36297255226915</td>\n",
       "      <td>0</td>\n",
       "      <td>1847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIVERTED</th>\n",
       "      <td>62946217</td>\n",
       "      <td>0.3684162782967561</td>\n",
       "      <td>6.376688817968346</td>\n",
       "      <td>0</td>\n",
       "      <td>1741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <td>62946157</td>\n",
       "      <td>133.9358472035076</td>\n",
       "      <td>75.2924727386293</td>\n",
       "      <td>-99</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <td>61827463</td>\n",
       "      <td>130.50674519832717</td>\n",
       "      <td>74.12851756858778</td>\n",
       "      <td>0</td>\n",
       "      <td>2206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIR_TIME</th>\n",
       "      <td>60438211</td>\n",
       "      <td>109.77697422248319</td>\n",
       "      <td>70.67954521240098</td>\n",
       "      <td>0</td>\n",
       "      <td>723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DISTANCE</th>\n",
       "      <td>61556964</td>\n",
       "      <td>786.6761085390762</td>\n",
       "      <td>593.670080015806</td>\n",
       "      <td>11</td>\n",
       "      <td>4983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <td>11390740</td>\n",
       "      <td>17.82056020943328</td>\n",
       "      <td>48.54477871808779</td>\n",
       "      <td>0</td>\n",
       "      <td>2439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <td>11390740</td>\n",
       "      <td>2.6575914295296004</td>\n",
       "      <td>20.60063143767953</td>\n",
       "      <td>0</td>\n",
       "      <td>2692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <td>11390740</td>\n",
       "      <td>14.403737246219297</td>\n",
       "      <td>29.66723364289759</td>\n",
       "      <td>0</td>\n",
       "      <td>1848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <td>11390740</td>\n",
       "      <td>0.07852861183733453</td>\n",
       "      <td>2.3879021507066964</td>\n",
       "      <td>0</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <td>11390740</td>\n",
       "      <td>23.283611424718675</td>\n",
       "      <td>42.74005368072112</td>\n",
       "      <td>0</td>\n",
       "      <td>2454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_YEAR</th>\n",
       "      <td>68979001</td>\n",
       "      <td>2014.0773005251265</td>\n",
       "      <td>3.251374041673039</td>\n",
       "      <td>2009</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_MONTH</th>\n",
       "      <td>68979001</td>\n",
       "      <td>6.516609496852528</td>\n",
       "      <td>3.3960196833407004</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_DOM</th>\n",
       "      <td>68979001</td>\n",
       "      <td>15.73120451541477</td>\n",
       "      <td>8.773566566277616</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL_DOW</th>\n",
       "      <td>68979001</td>\n",
       "      <td>3.9453938597922</td>\n",
       "      <td>1.959382874281212</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0                    1                   2      3  \\\n",
       "summary                 count                 mean              stddev    min   \n",
       "OP_CARRIER           68979001                 None                None     9E   \n",
       "OP_CARRIER_FL_NUM    68979001    2327.889195336418  1879.0626021936075      0   \n",
       "ORIGIN               68979001                 None                None    ABE   \n",
       "DEST                 68979001                 None                None    ABE   \n",
       "CRS_DEPT_TIME        68848914   1327.9867342279356   478.4238633024651    1.0   \n",
       "DEPT_TIME            67913168   1191.1727909085319   616.3955895624565  -82.0   \n",
       "DEP_DELAY            67904349    9.940476875788914  35.335181803239514   -251   \n",
       "TAXI_OUT             67881123   160.10510623107987  447.82081350655903      0   \n",
       "WHEELS_OFF           67877458    1367.072854879156  495.35591549375897    1.0   \n",
       "WHEELS_ON            67844338   1316.3517606141281    662.976840709899    1.0   \n",
       "TAXI_IN              67844340    163.4307853831285   484.5105195689285      0   \n",
       "CRS_ARR_TIME         68825194   1337.8248740134318   655.5450636017595  -99.0   \n",
       "ARR_TIME             67828181   1332.7721060218319   645.2998457732903    1.0   \n",
       "ARR_DELAY            67857650    91.74694898806545    318.563018352869   -411   \n",
       "CANCELLED            62946217   0.4817790559200722  10.299515858630517      0   \n",
       "CANCELLEATION_CODE    1389253   3.8024038817983477   32.36297255226915      0   \n",
       "DIVERTED             62946217   0.3684162782967561   6.376688817968346      0   \n",
       "CRS_ELAPSED_TIME     62946157    133.9358472035076    75.2924727386293    -99   \n",
       "ACTUAL_ELAPSED_TIME  61827463   130.50674519832717   74.12851756858778      0   \n",
       "AIR_TIME             60438211   109.77697422248319   70.67954521240098      0   \n",
       "DISTANCE             61556964    786.6761085390762    593.670080015806     11   \n",
       "CARRIER_DELAY        11390740    17.82056020943328   48.54477871808779      0   \n",
       "WEATHER_DELAY        11390740   2.6575914295296004   20.60063143767953      0   \n",
       "NAS_DELAY            11390740   14.403737246219297   29.66723364289759      0   \n",
       "SECURITY_DELAY       11390740  0.07852861183733453  2.3879021507066964      0   \n",
       "LATE_AIRCRAFT_DELAY  11390740   23.283611424718675   42.74005368072112      0   \n",
       "FL_YEAR              68979001   2014.0773005251265   3.251374041673039   2009   \n",
       "FL_MONTH             68979001    6.516609496852528  3.3960196833407004      1   \n",
       "FL_DOM               68979001    15.73120451541477   8.773566566277616      1   \n",
       "FL_DOW               68979001      3.9453938597922   1.959382874281212      1   \n",
       "\n",
       "                          4  \n",
       "summary                 max  \n",
       "OP_CARRIER               YX  \n",
       "OP_CARRIER_FL_NUM      9855  \n",
       "ORIGIN                  YUM  \n",
       "DEST                    YUM  \n",
       "CRS_DEPT_TIME        2400.0  \n",
       "DEPT_TIME            2710.0  \n",
       "DEP_DELAY              2755  \n",
       "TAXI_OUT               2400  \n",
       "WHEELS_OFF           2400.0  \n",
       "WHEELS_ON            2400.0  \n",
       "TAXI_IN                2400  \n",
       "CRS_ARR_TIME         2695.0  \n",
       "ARR_TIME             2400.0  \n",
       "ARR_DELAY              5095  \n",
       "CANCELLED              2695  \n",
       "CANCELLEATION_CODE     1847  \n",
       "DIVERTED               1741  \n",
       "CRS_ELAPSED_TIME       1865  \n",
       "ACTUAL_ELAPSED_TIME    2206  \n",
       "AIR_TIME                723  \n",
       "DISTANCE               4983  \n",
       "CARRIER_DELAY          2439  \n",
       "WEATHER_DELAY          2692  \n",
       "NAS_DELAY              1848  \n",
       "SECURITY_DELAY          987  \n",
       "LATE_AIRCRAFT_DELAY    2454  \n",
       "FL_YEAR                2019  \n",
       "FL_MONTH                 12  \n",
       "FL_DOM                   31  \n",
       "FL_DOW                    7  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156813ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop columns with high instances of null values\n",
    "cols = (\"CANCELLEATION_CODE\",\"CARRIER_DELAY\",\"WEATHER_DELAY\", \"NAS_DELAY\", \"SECURITY_DELAY\", \"LATE_AIRCRAFT_DELAY\")\n",
    "\n",
    "df = df.drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a817f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with null values\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7052dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  OP_CARRIER_FL_NUM 0.028855112459867296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  DEP_DELAY 0.9443134868300598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  TAXI_OUT 0.23593606727678315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  TAXI_IN 0.10613931498882627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  ARR_DELAY 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  CANCELLED nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  DIVERTED nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  CRS_ELAPSED_TIME -0.02571813167986106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  ACTUAL_ELAPSED_TIME 0.036192911879423105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  AIR_TIME -0.0016817726917779882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  DISTANCE -0.022357615383828064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  FL_YEAR 0.0037926677761505392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  FL_MONTH -0.010567785085853008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  FL_DOM 0.004068131837851167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:=====================================================>  (49 + 2) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation to ARR_DELAY for  FL_DOW -0.0021516113146673272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import six\n",
    "for i in df.columns:\n",
    "    if( isinstance(df.select(i).take(1)[0][0], six.integer_types)):\n",
    "        print( \"Correlation to ARR_DELAY for \", i, df.stat.corr('ARR_DELAY',i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea720d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "|            FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEPT_TIME|DEPT_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|FL_YEAR|FL_MONTH|FL_DOM|FL_DOW|OP_CARRIER_Index| OP_CARRIER_vec|OP_CARRIER_FL_NUM_Index|OP_CARRIER_FL_NUM_vec|ORIGIN_Index|      ORIGIN_vec|DEST_Index|        DEST_vec|\n",
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "|2013-01-01 00:00:00|        VX|              108|   LAX| IAD|        700.0|    700.0|        0|       8|     708.0|   1411.0|      7|      1445.0|  1418.0|      -27|        0|       0|             285|                258|     243|    2288|   2013|       1|     1|     3|            19.0|(22,[19],[1.0])|                  136.0|   (7599,[136],[1.0])|         4.0| (377,[4],[1.0])|      27.0|(376,[27],[1.0])|\n",
      "|2013-01-01 00:00:00|        VX|              114|   LAX| IAD|       2205.0|   2204.0|       -1|      12|    2216.0|    523.0|     13|       545.0|   536.0|       -9|        0|       0|             280|                272|     247|    2288|   2013|       1|     1|     3|            19.0|(22,[19],[1.0])|                  374.0|   (7599,[374],[1.0])|         4.0| (377,[4],[1.0])|      27.0|(376,[27],[1.0])|\n",
      "|2013-01-01 00:00:00|        VX|               11|   JFK| SFO|        730.0|    729.0|       -1|      18|     747.0|   1043.0|      6|      1115.0|  1049.0|      -26|        0|       0|             405|                380|     356|    2586|   2013|       1|     1|     3|            19.0|(22,[19],[1.0])|                  501.0|   (7599,[501],[1.0])|        18.0|(377,[18],[1.0])|       7.0| (376,[7],[1.0])|\n",
      "+-------------------+----------+-----------------+------+----+-------------+---------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------+--------+------+------+----------------+---------------+-----------------------+---------------------+------------+----------------+----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "categoricalColumns = [\"OP_CARRIER\",\"OP_CARRIER_FL_NUM\",\"ORIGIN\",\"DEST\"]\n",
    "\n",
    "OP_CARRIER_indexer = StringIndexer(inputCol=\"OP_CARRIER\", outputCol=\"OP_CARRIER_Index\")\n",
    "df_1 = OP_CARRIER_indexer.fit(df).transform(df)\n",
    "\n",
    "onehotencoder_OP_CARRIER_vector = OneHotEncoder(inputCol=\"OP_CARRIER_Index\", outputCol=\"OP_CARRIER_vec\")\n",
    "df_2 = onehotencoder_OP_CARRIER_vector.fit(df_1).transform(df_1)\n",
    "\n",
    "OP_CARRIER_FL_NUM_indexer = StringIndexer(inputCol=\"OP_CARRIER_FL_NUM\", outputCol=\"OP_CARRIER_FL_NUM_Index\")\n",
    "df_3 = OP_CARRIER_FL_NUM_indexer.fit(df_2).transform(df_2)\n",
    "\n",
    "onehotencoder_OP_CARRIER_FL_NUM_vector = OneHotEncoder(inputCol=\"OP_CARRIER_FL_NUM_Index\", outputCol=\"OP_CARRIER_FL_NUM_vec\")\n",
    "df_4 = onehotencoder_OP_CARRIER_FL_NUM_vector.fit(df_3).transform(df_3)\n",
    "\n",
    "ORIGIN_indexer = StringIndexer(inputCol=\"ORIGIN\", outputCol=\"ORIGIN_Index\")\n",
    "df_5 = ORIGIN_indexer.fit(df_4).transform(df_4)\n",
    "\n",
    "onehotencoder_ORIGIN_vector = OneHotEncoder(inputCol=\"ORIGIN_Index\", outputCol=\"ORIGIN_vec\")\n",
    "df_6 = onehotencoder_ORIGIN_vector.fit(df_5).transform(df_5)\n",
    "\n",
    "DEST_indexer = StringIndexer(inputCol=\"DEST\", outputCol=\"DEST_Index\")\n",
    "df_7 = DEST_indexer.fit(df_6).transform(df_6)\n",
    "\n",
    "onehotencoder_DEST_vector = OneHotEncoder(inputCol=\"DEST_Index\", outputCol=\"DEST_vec\")\n",
    "df_8 = onehotencoder_DEST_vector.fit(df_7).transform(df_7)\n",
    "df_8.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc31a225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|            features|ARR_DELAY|\n",
      "+--------------------+---------+\n",
      "|(8386,[19,158,762...|      -27|\n",
      "|(8386,[19,396,762...|       -9|\n",
      "|(8386,[19,523,763...|      -26|\n",
      "+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = ['OP_CARRIER_vec', 'OP_CARRIER_FL_NUM_vec', 'ORIGIN_vec','DEST_vec',\n",
    "                                               'TAXI_OUT', 'TAXI_IN', 'CANCELLED','DIVERTED','CRS_ELAPSED_TIME',\n",
    "                                               'ACTUAL_ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'FL_YEAR', 'FL_MONTH',\n",
    "                                               'FL_DOM', 'FL_DOW'], outputCol = 'features')\n",
    "v_df = vectorAssembler.transform(df_8)\n",
    "v_df = v_df.select(['features', 'ARR_DELAY'])\n",
    "v_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b532e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = v_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249b0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T00:04:31,901 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1311.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T00:11:06,046 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1312.3 KiB\n",
      "2022-12-15T00:11:06,943 WARN [dag-scheduler-event-loop] org.apache.spark.scheduler.DAGScheduler - Broadcasting large task binary with size 1312.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:>                                                        (0 + 8) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T00:12:06,386 WARN [Executor task launch worker for task 0.0 in stage 88.0 (TID 1361)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_334_0 in memory! (computed 33.0 MiB so far)\n",
      "2022-12-15T00:12:06,400 WARN [Executor task launch worker for task 0.0 in stage 88.0 (TID 1361)] org.apache.spark.storage.BlockManager - Persisting block rdd_334_0 to disk instead.\n",
      "2022-12-15T00:12:06,700 WARN [Executor task launch worker for task 5.0 in stage 88.0 (TID 1366)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_334_5 in memory! (computed 33.0 MiB so far)\n",
      "2022-12-15T00:12:06,700 WARN [Executor task launch worker for task 5.0 in stage 88.0 (TID 1366)] org.apache.spark.storage.BlockManager - Persisting block rdd_334_5 to disk instead.\n",
      "2022-12-15T00:12:06,759 WARN [Executor task launch worker for task 2.0 in stage 88.0 (TID 1363)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_334_2 in memory! (computed 33.0 MiB so far)\n",
      "2022-12-15T00:12:06,760 WARN [Executor task launch worker for task 2.0 in stage 88.0 (TID 1363)] org.apache.spark.storage.BlockManager - Persisting block rdd_334_2 to disk instead.\n",
      "2022-12-15T00:12:06,777 WARN [Executor task launch worker for task 4.0 in stage 88.0 (TID 1365)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_334_4 in memory! (computed 33.0 MiB so far)\n",
      "2022-12-15T00:12:06,778 WARN [Executor task launch worker for task 4.0 in stage 88.0 (TID 1365)] org.apache.spark.storage.BlockManager - Persisting block rdd_334_4 to disk instead.\n",
      "2022-12-15T00:12:06,817 WARN [Executor task launch worker for task 1.0 in stage 88.0 (TID 1362)] org.apache.spark.storage.memory.MemoryStore - Not enough space to cache rdd_334_1 in memory! (computed 33.0 MiB so far)\n",
      "2022-12-15T00:12:06,818 WARN [Executor task launch worker for task 1.0 in stage 88.0 (TID 1362)] org.apache.spark.storage.BlockManager - Persisting block rdd_334_1 to disk instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:>                                                        (0 + 8) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T00:12:35,739 WARN [Executor task launch worker for task 7.0 in stage 88.0 (TID 1368)] org.apache.spark.storage.BlockManager - Block rdd_334_7 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T00:12:36,268 ERROR [Executor task launch worker for task 7.0 in stage 88.0 (TID 1368)] org.apache.spark.executor.Executor - Exception in task 7.0 in stage 88.0 (TID 1368)\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Integer.valueOf(Integer.java:832) ~[?:1.8.0_352]\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621) ~[spark-mllib-local_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) ~[?:?]\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "2022-12-15T00:12:36,900 ERROR [Executor task launch worker for task 7.0 in stage 88.0 (TID 1368)] org.apache.spark.util.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 88.0 (TID 1368),5,main]\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Integer.valueOf(Integer.java:832) ~[?:1.8.0_352]\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621) ~[spark-mllib-local_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source) ~[?:?]\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151) ~[spark-mllib_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source) ~[?:?]\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 88:>                                                        (0 + 9) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15T00:12:38,350 WARN [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Lost task 7.0 in stage 88.0 (TID 1368) (192.168.0.115 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Integer.valueOf(Integer.java:832)\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n",
      "\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621)\n",
      "\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\n",
      "2022-12-15T00:12:38,355 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 7 in stage 88.0 failed 1 times; aborting job\n",
      "2022-12-15T00:12:38,936 WARN [Executor task launch worker for task 3.0 in stage 88.0 (TID 1364)] org.apache.spark.storage.BlockManager - Putting block rdd_334_3 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T00:12:38,937 WARN [Executor task launch worker for task 3.0 in stage 88.0 (TID 1364)] org.apache.spark.storage.BlockManager - Block rdd_334_3 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T00:12:38,941 WARN [Executor task launch worker for task 4.0 in stage 88.0 (TID 1365)] org.apache.spark.storage.BlockManager - Putting block rdd_334_4 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T00:12:38,942 WARN [Executor task launch worker for task 4.0 in stage 88.0 (TID 1365)] org.apache.spark.storage.BlockManager - Block rdd_334_4 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T00:12:38,942 WARN [Executor task launch worker for task 6.0 in stage 88.0 (TID 1367)] org.apache.spark.storage.BlockManager - Putting block rdd_334_6 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T00:12:38,945 WARN [Executor task launch worker for task 6.0 in stage 88.0 (TID 1367)] org.apache.spark.storage.BlockManager - Block rdd_334_6 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T00:12:39,386 WARN [Executor task launch worker for task 0.0 in stage 88.0 (TID 1361)] org.apache.spark.storage.BlockManager - Putting block rdd_334_0 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T00:12:39,389 WARN [Executor task launch worker for task 0.0 in stage 88.0 (TID 1361)] org.apache.spark.storage.BlockManager - Block rdd_334_0 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T00:12:39,393 ERROR [Thread-20] org.apache.spark.ml.util.Instrumentation - org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 88.0 failed 1 times, most recent failure: Lost task 7.0 in stage 88.0 (TID 1368) (192.168.0.115 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Integer.valueOf(Integer.java:832)\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n",
      "\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621)\n",
      "\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainImpl(LinearRegression.scala:578)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:422)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.lang.Integer.valueOf(Integer.java:832)\n",
      "\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n",
      "\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621)\n",
      "\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164)\n",
      "\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n",
      "\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\n",
      "2022-12-15T00:12:39,402 WARN [Executor task launch worker for task 2.0 in stage 88.0 (TID 1363)] org.apache.spark.storage.BlockManager - Putting block rdd_334_2 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T00:12:39,404 WARN [Executor task launch worker for task 2.0 in stage 88.0 (TID 1363)] org.apache.spark.storage.BlockManager - Block rdd_334_2 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T00:12:39,417 WARN [task-result-getter-3] org.apache.spark.scheduler.TaskSetManager - Lost task 4.0 in stage 88.0 (TID 1365) (192.168.0.115 executor driver): TaskKilled (Stage cancelled)\n",
      "2022-12-15T00:12:39,420 WARN [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Lost task 3.0 in stage 88.0 (TID 1364) (192.168.0.115 executor driver): TaskKilled (Stage cancelled)\n",
      "2022-12-15T00:12:39,429 WARN [Executor task launch worker for task 5.0 in stage 88.0 (TID 1366)] org.apache.spark.storage.BlockManager - Putting block rdd_334_5 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T00:12:39,431 WARN [Executor task launch worker for task 5.0 in stage 88.0 (TID 1366)] org.apache.spark.storage.BlockManager - Block rdd_334_5 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T00:12:39,435 WARN [Executor task launch worker for task 1.0 in stage 88.0 (TID 1362)] org.apache.spark.storage.BlockManager - Putting block rdd_334_1 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T00:12:39,437 WARN [Executor task launch worker for task 1.0 in stage 88.0 (TID 1362)] org.apache.spark.storage.BlockManager - Block rdd_334_1 could not be removed as it was not found on disk or in memory\n",
      "2022-12-15T00:12:39,446 WARN [task-result-getter-2] org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 88.0 (TID 1361) (192.168.0.115 executor driver): TaskKilled (Stage cancelled)\n",
      "2022-12-15T00:12:39,450 WARN [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Lost task 6.0 in stage 88.0 (TID 1367) (192.168.0.115 executor driver): TaskKilled (Stage cancelled)\n",
      "2022-12-15T00:12:39,468 ERROR [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSchedulerImpl - Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4597/1266773909@682f6a90 rejected from java.util.concurrent.ThreadPoolExecutor@2fcd23cc[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1366]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) ~[?:1.8.0_352]\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_352]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_352]\n",
      "2022-12-15T00:12:39,469 ERROR [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSchedulerImpl - Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4597/1266773909@32ef6cf7 rejected from java.util.concurrent.ThreadPoolExecutor@2fcd23cc[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1366]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) ~[?:1.8.0_352]\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_352]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_352]\n",
      "2022-12-15T00:12:39,470 ERROR [dispatcher-event-loop-1] org.apache.spark.scheduler.TaskSchedulerImpl - Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$4597/1266773909@2107ebd0 rejected from java.util.concurrent.ThreadPoolExecutor@2fcd23cc[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1366]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379) ~[?:1.8.0_352]\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:137) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:821) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:794) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41) ~[spark-core_2.12-3.3.1.jar:3.3.1]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_352]\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_352]\n",
      "\tat java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_352]\n",
      "2022-12-15T00:12:39,717 WARN [Executor task launch worker for task 8.0 in stage 88.0 (TID 1369)] org.apache.spark.storage.BlockManager - Putting block rdd_334_8 failed due to exception org.apache.spark.TaskKilledException.\n",
      "2022-12-15T00:12:39,718 WARN [Executor task launch worker for task 8.0 in stage 88.0 (TID 1369)] org.apache.spark.storage.BlockManager - Block rdd_334_8 could not be removed as it was not found on disk or in memory\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o633.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 88.0 failed 1 times, most recent failure: Lost task 7.0 in stage 88.0 (TID 1368) (192.168.0.115 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.Integer.valueOf(Integer.java:832)\n\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621)\n\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164)\n\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\n\tat org.apache.spark.ml.regression.LinearRegression.trainImpl(LinearRegression.scala:578)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:422)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.Integer.valueOf(Integer.java:832)\n\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621)\n\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164)\n\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m      2\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression(featuresCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARR_DELAY\u001b[39m\u001b[38;5;124m'\u001b[39m, maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, regParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, elasticNetParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoefficients: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lr_model\u001b[38;5;241m.\u001b[39mcoefficients))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntercept: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(lr_model\u001b[38;5;241m.\u001b[39mintercept))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/ml/wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o633.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 88.0 failed 1 times, most recent failure: Lost task 7.0 in stage 88.0 (TID 1368) (192.168.0.115 executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.Integer.valueOf(Integer.java:832)\n\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621)\n\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164)\n\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:50)\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:44)\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:96)\n\tat org.apache.spark.ml.regression.LinearRegression.trainImpl(LinearRegression.scala:578)\n\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:422)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:327)\n\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:184)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.lang.Integer.valueOf(Integer.java:832)\n\tat scala.runtime.BoxesRunTime.boxToInteger(BoxesRunTime.java:67)\n\tat scala.collection.mutable.ArrayOps$ofInt.apply(ArrayOps.scala:246)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n\tat org.apache.spark.ml.linalg.SparseVector.<init>(Vectors.scala:621)\n\tat org.apache.spark.ml.linalg.VectorUDT.deserialize(VectorUDT.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.CreateExternalRow_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:164)\n\tat org.apache.spark.ml.feature.InstanceBlock$$anon$1.next(Instance.scala:151)\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:224)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1518)\n\tat org.apache.spark.storage.BlockManager$$Lambda$2186/401964946.apply(Unknown Source)\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1445)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1509)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1332)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='ARR_DELAY', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
